{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## SIF (Smooth Inverse Frequency) Embeddings\n",
    "\n",
    "SIF is a simple sentence embedding technique. The sentence embeddings are calculated based on the weighted average of word embeddings.\n",
    "\n",
    "The utility and SIF calculation functions are refactored from https://github.com/PrincetonML/SIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuation_list(exclude_puncts=None, include_puncts=None):\n",
    "    \"\"\" Get a customized punctuation list \"\"\"\n",
    "    if not exclude_puncts:\n",
    "        exclude_puncts = []\n",
    "    if not include_puncts:\n",
    "        include_puncts = []\n",
    "    puncts = [p for p in string.punctuation if p not in exclude_puncts]\n",
    "    puncts.extend([p for p in include_puncts])\n",
    "    return list(set(puncts))\n",
    "\n",
    "def separate_punctuation(x, puncts):\n",
    "    \"\"\" Add spaces around punctuations \"\"\"\n",
    "    for s in puncts:\n",
    "        x = x.replace(s, f' {s} ')\n",
    "    return x\n",
    "\n",
    "def strip_space(x):\n",
    "    \"\"\" Remove extra space around words \"\"\"\n",
    "    return ' '.join([s.strip() for s in x.split()])\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace('\\n', '').replace('\\t', '')\n",
    "    x = separate_punctuation(x, puncts)\n",
    "    x = strip_space(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and format functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_to_sequence(X_train, X_test, **kwargs):\n",
    "    \"\"\"\n",
    "    Process text data (array) to equal length sequences use keras\n",
    "    :param X_train : np.array with shape (m, )\n",
    "    :param X_test  : np.array with shape (n, )\n",
    "    :param kwargs  : other parameters needed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_features = kwargs.get('max_features', 10000)\n",
    "    max_len = kwargs.get('max_len', 50)\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features, lower=True, split=' ',\n",
    "                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                               char_level=False)\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "\n",
    "    # process text to sequence\n",
    "    X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # truncate / padding\n",
    "    X_train_sequence_pad = sequence.pad_sequences(X_train_sequence, maxlen=max_len)\n",
    "    X_test_sequence_pad = sequence.pad_sequences(X_test_sequence, maxlen=max_len)\n",
    "\n",
    "    return X_train_sequence, X_test_sequence, X_train_sequence_pad, X_test_sequence_pad, tokenizer\n",
    "\n",
    "def load_pretrained_word_embeddings(embedding_path, tokenizer, **kwargs):\n",
    "    \"\"\"\n",
    "    Load pretrained word embeddings\n",
    "    :param embedding_path : str, example: './embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    :param tokenizer      : keras tokenizer, return from process_text_to_sequence\n",
    "    :param kwargs         : other parameters needed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_size = kwargs.get('embedding_size', 300)\n",
    "    max_features = kwargs.get('max_features', 10000)\n",
    "    \n",
    "    # word_pretrained_index: key = word, value = index in pretrained embeddings\n",
    "    word_pretrained_index = {}\n",
    "    \n",
    "    def _get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(\n",
    "        _get_coefs(*o.strip().rsplit(' ')) for o in open(embedding_path))\n",
    "    embeddings_vocab = list(embeddings_index.keys())\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index))\n",
    "    embeddings_matrix = np.zeros((num_words, embedding_size))\n",
    "    for word, i in word_index.items():\n",
    "        i -= 1\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embeddings_vector = embeddings_index.get(word)\n",
    "        # oov or not\n",
    "        if embeddings_vector is not None:\n",
    "            embeddings_matrix[i] = embeddings_vector\n",
    "            word_pretrained_index[word] = embeddings_vocab.index(word)\n",
    "        \n",
    "        # make 0 as start\n",
    "        word_index[word] -= 1\n",
    "        \n",
    "    return word_pretrained_index, word_index, embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions (get term-frequency weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_weight(weight_filename, alpha=1e-3):\n",
    "    \"\"\" Get word term-frequency from large corpus file \"\"\"\n",
    "    # when the parameter makes no sense, use unweighted\n",
    "    if alpha <= 0: \n",
    "        alpha = 1.0\n",
    "    \n",
    "    words_weight = {}\n",
    "    with open(weight_filename) as f:\n",
    "        lines = f.readlines()\n",
    "     \n",
    "    # total term-frequency\n",
    "    N = 0\n",
    "    for word_tf in lines:\n",
    "        word_tf = word_tf.strip()\n",
    "        if len(word_tf) == 0:\n",
    "            continue\n",
    "        word_tf = word_tf.split()\n",
    "        if len(word_tf) == 2:\n",
    "            word = word_tf[0]\n",
    "            tf = word_tf[1]\n",
    "            words_weight[word] = float(tf)\n",
    "            N += float(tf)\n",
    "        else:\n",
    "            print('{} not a valid (word, termfrequency) record'.format(i))\n",
    "                \n",
    "    # normalize weights by alpha and N\n",
    "    for word, tf in words_weight.items():\n",
    "        words_weight[word] = alpha / (alpha + tf / N)\n",
    "        \n",
    "    return words_weight\n",
    "\n",
    "def lookup_pretrained_index(words_pretrained_index, word):\n",
    "    word = word.lower()\n",
    "    if len(word) > 1 and word[0] == '#':\n",
    "        word = word.replace(\"#\", \"\")\n",
    "        \n",
    "    if word in words_pretrained_index:\n",
    "        return words_pretrained_index[word]\n",
    "    elif 'UUUNKKK' in words_pretrained_index:\n",
    "        return words_pretrained_index['UUUNKKK']\n",
    "    else:\n",
    "        return len(words_pretrained_index) - 1\n",
    "\n",
    "def get_pretrained_index_weight(word_pretrained_index, words_weight):\n",
    "    \"\"\" Get the map from word index in pretrained embeddings and weights \"\"\"\n",
    "    index_weights = {}\n",
    "    for word, idx in word_pretrained_index.items():\n",
    "        if word in words_weight:\n",
    "            index_weights[idx] = words_weight[word]\n",
    "        else:\n",
    "            index_weights[idx] = 1.0\n",
    "    return index_weights\n",
    "\n",
    "def get_sentence_pretrained_index(sentences, words_pretrained_index):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices \n",
    "    that can be fed into the algorithms.\n",
    "    Since sentences have different length, 0 will be padded at \n",
    "    the end for sentence length less than max length\n",
    "    \n",
    "    :param sentences: \n",
    "    :param words_pretrained_index:\n",
    "    :return: word_index_sentence, mask. \n",
    "             word_index_sentence[i, :] is the word indices in sentence i\n",
    "             mask[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    def get_sequence(sentence, words_pretrained_index):\n",
    "        return [lookup_pretrained_index(words_pretrained_index, word) for word in sentence.split()]\n",
    "\n",
    "    sequence = [get_sequence(sentence, words_pretrained_index) for sentence in sentences]\n",
    "    word_index_sentence, mask = pad_sequences(sequence)\n",
    "    return word_index_sentence, mask\n",
    "\n",
    "def pad_sequences(sequences):\n",
    "    \"\"\" Padding 0 to sequences that shorter than max length \"\"\"\n",
    "    lengths = [len(s) for s in sequences]\n",
    "    n_samples = len(sequences)\n",
    "    maxlen = np.max(lengths)\n",
    "    \n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    for idx, sentence in enumerate(sequences):\n",
    "        x[idx, :lengths[idx]] = sentence\n",
    "        x_mask[idx, :lengths[idx]] = 1.\n",
    "    x_mask = np.asarray(x_mask, dtype='float32')\n",
    "    return x, x_mask\n",
    "\n",
    "def get_word_weights_sequence(sequences, mask, index_weights):\n",
    "    \"\"\" Get word weights for sentences \"\"\"\n",
    "    weight = np.zeros(sequences.shape).astype('float32')\n",
    "    \n",
    "    for i in range(sequences.shape[0]):\n",
    "        for j in range(sequences.shape[1]):\n",
    "            if mask[i, j] > 0 and sequences[i, j] >= 0:\n",
    "                weight[i, j] = index_weights[sequences[i, j]]\n",
    "    \n",
    "    weight = np.asarray(weight, dtype='float32')\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIF embedding calculation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_average(embedding_matrix, word_index_sentence, weights):\n",
    "    \"\"\" Compute the weighted average word embeddings \"\"\"\n",
    "    n_samples = word_index_sentence.shape[0]\n",
    "    embedding_matrix_avg = np.zeros((n_samples, embedding_matrix.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        total_weights = np.count_nonzero(weights[i, :])\n",
    "        embedding_matrix_avg[i,:] = weights[i, :].dot(embedding_matrix[word_index_sentence[i, :], :]) / total_weights\n",
    "    return embedding_matrix_avg\n",
    "\n",
    "def compute_pc(X, n_components=1, **kwargs):\n",
    "    \"\"\" Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN! \"\"\"\n",
    "    svd = TruncatedSVD(n_components=n_components, **kwargs)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, n_components_rm=1, **kwargs):\n",
    "    \"\"\" Remove the projection on the principal components \"\"\"\n",
    "    n_components = kwargs.get('n_components', 1)\n",
    "    pc = compute_pc(X, n_components, random_state=2020)\n",
    "    if n_components_rm == 1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "def sif_embeddings(embeddings_matrix, word_index_sentence, weights):\n",
    "    \"\"\" Get SIF embeddings \"\"\"\n",
    "    embeddings_matrix_avg = get_weighted_average(embeddings_matrix, word_index_sentence, weights)\n",
    "    embeddings_matrix_avg_rm_pc = remove_pc(embeddings_matrix_avg)\n",
    "    return embeddings_matrix_avg_rm_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor ( koh-i-noor ) d...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonely ? how can i solv...</td>\n",
       "      <td>find the remainder when [ math ] 23 ^ { 24 } [...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar , sal...</td>\n",
       "      <td>which fish would survive in salt water ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4  what is the story of kohinoor ( koh-i-noor ) d...   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8  why am i mentally very lonely ? how can i solv...   \n",
       "4   4     9    10  which one dissolve in water quikly sugar , sal...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  what is the step by step guide to invest in sh...             0  \n",
       "1  what would happen if the indian government sto...             0  \n",
       "2  how can internet speed be increased by hacking...             0  \n",
       "3  find the remainder when [ math ] 23 ^ { 24 } [...             0  \n",
       "4           which fish would survive in salt water ?             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../input/quora-question-pairs/'\n",
    "word_embedding_file = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "word_weight_file = '../input/enwiki-vocab-min200//enwiki_vocab_min200.txt'\n",
    "weight_para = 1e-3\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_path, 'train.csv.zip'))\n",
    "train[['question1', 'question2']].fillna('', inplace=True)\n",
    "\n",
    "puncts = get_punctuation_list(exclude_puncts=['$', '-', '_'])\n",
    "train['question1'] = train['question1'].apply(clean_text)\n",
    "train['question2'] = train['question2'].apply(clean_text)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808580,) (2,)\n"
     ]
    }
   ],
   "source": [
    "train_size = train.shape[0]\n",
    "sentences = train['question1'].tolist() + train['question2'].tolist()\n",
    "X_train = np.array(sentences)\n",
    "X_test = np.array(sentences[:2])\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, tokenizer = process_text_to_sequence(X_train, X_test, max_features=1000000, max_len=100)\n",
    "word_pretrained_index, word_index, embeddings_matrix = load_pretrained_word_embeddings(word_embedding_file,\n",
    "                                                                   tokenizer,\n",
    "                                                                   max_features=1000000,\n",
    "                                                                   max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weight = get_word_weight(word_weight_file, weight_para)\n",
    "index_weights = get_pretrained_index_weight(word_index, word_weight)\n",
    "word_index_sentence, mask = get_sentence_pretrained_index(sentences, word_index)\n",
    "weights = get_word_weights_sequence(word_index_sentence, mask, index_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808580, 300)\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = get_weighted_average(embeddings_matrix, word_index_sentence, weights)\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 300) (404290, 300)\n"
     ]
    }
   ],
   "source": [
    "question1_embeddings = sentence_embeddings[:train_size]\n",
    "question2_embeddings = sentence_embeddings[train_size:]\n",
    "print(question1_embeddings.shape, question2_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "for i in range(train_size):\n",
    "    sentence_1 = question1_embeddings[i]\n",
    "    sentence_2 = question2_embeddings[i]\n",
    "    cosine_similarities.append(1 - cosine(sentence_1, sentence_2))\n",
    "    \n",
    "train['cosine_similarity'] = cosine_similarities\n",
    "train.to_csv('train_questions_sif_cosine_similarity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_params = dict(kind='hist', bins=200, alpha=0.5, figsize=(15, 6), density=True)\n",
    "ax = train.loc[train['is_duplicate'] == 0, 'cosine_similarity'].plot(color='blue', **plot_params)\n",
    "_ = train.loc[train['is_duplicate'] == 1, 'cosine_similarity'].plot(color='red', ax=ax, **plot_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
