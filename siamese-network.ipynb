{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "\n",
    "Use Quora Question Pairs competition data and pretrained word embeddings (GloVe), build a simple siamese network.\n",
    "\n",
    "Siamese network is a type of neural network that uses same weight for two different inputs to get a comparison. Here is the example of using siamese network on supervised text similarity task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, Embedding, LSTM, CuDNNLSTM, Dense, Layer, Lambda\n",
    "from keras.layers import concatenate, subtract, add, maximum, multiply\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def process_text_to_sequence(X_train, X_test, **kwargs):\n",
    "    \"\"\"\n",
    "    Process text data (array) to equal length sequences use keras\n",
    "    :param X_train : np.array with shape (m, )\n",
    "    :param X_test  : np.array with shape (n, )\n",
    "    :param kwargs  : other parameters needed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_features = kwargs.get('max_features', 10000)\n",
    "    max_len = kwargs.get('max_len', 50)\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features, lower=True, split=' ',\n",
    "                               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                               char_level=False)\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "\n",
    "    # process text to sequence\n",
    "    X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # truncate / padding\n",
    "    X_train_sequence_pad = sequence.pad_sequences(X_train_sequence, maxlen=max_len)\n",
    "    X_test_sequence_pad = sequence.pad_sequences(X_test_sequence, maxlen=max_len)\n",
    "\n",
    "    return dict(X_train_sequence=X_train_sequence,\n",
    "                X_test_sequence=X_test_sequence,\n",
    "                X_train_sequence_pad=X_train_sequence_pad,\n",
    "                X_test_sequence_pad=X_test_sequence_pad,\n",
    "                tokenizer=tokenizer)\n",
    "\n",
    "def load_pretrained_word_embeddings(embedding_path, tokenizer, **kwargs):\n",
    "    \"\"\"\n",
    "    Load pretrained word embeddings\n",
    "    :param embedding_path : str, example: './embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    :param tokenizer      : keras tokenizer, return from process_text_to_sequence\n",
    "    :param kwargs         : other parameters needed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_size = kwargs.get('embedding_size', 300)\n",
    "    max_features = kwargs.get('max_features', 10000)\n",
    "    \n",
    "    # word_pretrained_index: key = word, value = index in pretrained embeddings\n",
    "    word_pretrained_index = {}\n",
    "    \n",
    "    def _get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(\n",
    "        _get_coefs(*o.strip().rsplit(' ')) for o in open(embedding_path))\n",
    "    embeddings_vocab = list(embeddings_index.keys())\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index))\n",
    "    embeddings_matrix = np.zeros((num_words, embedding_size))\n",
    "    for word, i in word_index.items():\n",
    "        i -= 1\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embeddings_vector = embeddings_index.get(word)\n",
    "        # oov or not\n",
    "        if embeddings_vector is not None:\n",
    "            embeddings_matrix[i] = embeddings_vector\n",
    "            word_pretrained_index[word] = embeddings_vocab.index(word)\n",
    "        \n",
    "        # make 0 as start\n",
    "        word_index[word] -= 1\n",
    "        \n",
    "    return dict(word_pretrained_index=word_pretrained_index,\n",
    "                word_index=word_index,\n",
    "                embeddings_matrix=embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290,) (404290,) (3563475,) (3563475,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../input/quora-question-pairs/'\n",
    "word_embedding_file = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_path, 'train.csv.zip'))\n",
    "test = pd.read_csv(os.path.join(data_path, 'test.csv.zip'))\n",
    "\n",
    "X_train_q1 = train['question1'].fillna('').values\n",
    "X_train_q2 = train['question2'].fillna('').values\n",
    "X_test_q1 = test['question1'].fillna('').values\n",
    "X_test_q2 = test['question2'].fillna('').values\n",
    "y_train = train['is_duplicate'].values\n",
    "print(X_train_q1.shape, X_train_q2.shape, X_test_q1.shape, X_test_q2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323432, 40) (80858, 40) (323432, 40) (80858, 40)\n"
     ]
    }
   ],
   "source": [
    "train_size = train.shape[0]\n",
    "X_train_full = np.hstack([X_train_q1, X_train_q2])\n",
    "X_test_full = np.hstack([X_test_q1, X_test_q2])\n",
    "\n",
    "output = process_text_to_sequence(X_train_full, X_test_full, max_features=None, max_len=40)\n",
    "\n",
    "X_train_q1_, X_valid_q1_, X_train_q2_, X_valid_q2_, y_train_, y_valid_ = train_test_split(\n",
    "    output['X_train_sequence_pad'][:train_size],\n",
    "    output['X_train_sequence_pad'][train_size:],\n",
    "    y_train, train_size=0.8, random_state=2020)\n",
    "\n",
    "X_train_ = [X_train_q1_, X_train_q2_]\n",
    "X_valid_ = [X_valid_q1_, X_valid_q2_]\n",
    "\n",
    "print(X_train_q1_.shape, X_valid_q1_.shape, X_train_q2_.shape, X_valid_q2_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embeddings = load_pretrained_word_embeddings(word_embedding_file,\n",
    "                                                    output['tokenizer'],\n",
    "                                                    max_features=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_params):\n",
    "    \"\"\" Build siamese model \"\"\"\n",
    "    input_size = model_params['input_size']\n",
    "    output_size = model_params['output_size']\n",
    "    max_features = model_params['max_features']\n",
    "    embeddings_size = model_params['embeddings_size']\n",
    "    embeddings_matrix = model_params['embeddings_matrix']\n",
    "    loss = model_params['loss']\n",
    "    optimizer = model_params['optimizer']\n",
    "    metrics = model_params['metrics']\n",
    "    \n",
    "    input_q1 = Input(shape=input_size)\n",
    "    input_q2 = Input(shape=input_size)    \n",
    "    x_1 = input_q1\n",
    "    x_2 = input_q2\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=max_features,\n",
    "                    output_dim=embeddings_size,\n",
    "                    weights=[embeddings_matrix],\n",
    "                    trainable=False)\n",
    "    x_1 = embedding_layer(x_1)\n",
    "    x_2 = embedding_layer(x_2)\n",
    "    \n",
    "    lstm_layer = LSTM(64, return_sequences=False)\n",
    "    x_1 = lstm_layer(x_1)\n",
    "    x_2 = lstm_layer(x_2)\n",
    "    \n",
    "    x = subtract([x_1, x_2])\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    output = Dense(units=output_size, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_q1, input_q2], outputs=output)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNN(ABC):\n",
    "    @abstractmethod\n",
    "    def initialize(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        pass\n",
    "    \n",
    "class SiameseNetwork(BaseNN):\n",
    "    def __init__(self, model_params):\n",
    "        self.model_params = model_params\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\" Initialize model \"\"\"\n",
    "        model = make_model(self.model_params)\n",
    "        print('Model Summary:')\n",
    "        print(model.summary())\n",
    "        self._model = model\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        assert self._model is not None\n",
    "        self.train_params = self.model_params['train_params']\n",
    "        self.patience = self.train_params['patience']\n",
    "        self.nb_epochs = self.train_params['nb_epochs']\n",
    "        self.batch_size = self.train_params['batch_size']\n",
    "        self.filepath = self.model_params['filepath']\n",
    "        earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                                      patience=self.patience,\n",
    "                                      mode='min')\n",
    "        checkpointer = ModelCheckpoint(filepath=self.filepath, save_best_only=True)\n",
    "        history = self._model.fit(X_train, y_train,\n",
    "                                  epochs=self.nb_epochs,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  validation_data=(X_val, y_val),\n",
    "                                  callbacks=[earlystopping, checkpointer]).history\n",
    "        return self._model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            if self.saved_model_destination is None:\n",
    "                return self._model.predict(X)\n",
    "            else:\n",
    "                loaded_model = load_model(self.saved_model_destination)\n",
    "                return loaded_model.predict(X)\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\"Model not saved, try .save() first.\")\n",
    "            \n",
    "    def save(self, saved_model_destination):\n",
    "        assert self._model is not None\n",
    "        self.saved_model_destination = saved_model_destination\n",
    "        self._model.save(self.saved_model_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size'  : (40, ), # = max_len\n",
    "    'output_size' : 1,\n",
    "    'train_params': {'batch_size': 512,\n",
    "                     'patience'  : 2,\n",
    "                     'nb_epochs' : 10},\n",
    "    'max_features': output_embeddings['embeddings_matrix'].shape[0],\n",
    "    'embeddings_size': 300,\n",
    "    'embeddings_matrix': output_embeddings['embeddings_matrix'],\n",
    "    'loss'     : 'binary_crossentropy',\n",
    "    'optimizer': optimizers.Adam(),\n",
    "    'metrics'  : ['accuracy'],\n",
    "    'filepath' : './model.h5'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 300)      41112600    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           93440       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 64)           0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            65          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,210,265\n",
      "Trainable params: 97,665\n",
      "Non-trainable params: 41,112,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/10\n",
      "323432/323432 [==============================] - 227s 701us/step - loss: 0.5653 - accuracy: 0.6974 - val_loss: 0.5239 - val_accuracy: 0.7394\n",
      "Epoch 2/10\n",
      "323432/323432 [==============================] - 236s 730us/step - loss: 0.4923 - accuracy: 0.7586 - val_loss: 0.4790 - val_accuracy: 0.7686\n",
      "Epoch 3/10\n",
      "323432/323432 [==============================] - 223s 691us/step - loss: 0.4536 - accuracy: 0.7837 - val_loss: 0.4642 - val_accuracy: 0.7797\n",
      "Epoch 4/10\n",
      "323432/323432 [==============================] - 236s 731us/step - loss: 0.4268 - accuracy: 0.8005 - val_loss: 0.4483 - val_accuracy: 0.7880\n",
      "Epoch 5/10\n",
      "323432/323432 [==============================] - 232s 718us/step - loss: 0.4049 - accuracy: 0.8124 - val_loss: 0.4408 - val_accuracy: 0.7958\n",
      "Epoch 6/10\n",
      "323432/323432 [==============================] - 233s 721us/step - loss: 0.3869 - accuracy: 0.8232 - val_loss: 0.4361 - val_accuracy: 0.7968\n",
      "Epoch 7/10\n",
      "323432/323432 [==============================] - 232s 717us/step - loss: 0.3714 - accuracy: 0.8320 - val_loss: 0.4349 - val_accuracy: 0.7992\n",
      "Epoch 8/10\n",
      "323432/323432 [==============================] - 236s 729us/step - loss: 0.3572 - accuracy: 0.8400 - val_loss: 0.4329 - val_accuracy: 0.8024\n",
      "Epoch 9/10\n",
      "323432/323432 [==============================] - 236s 729us/step - loss: 0.3440 - accuracy: 0.8474 - val_loss: 0.4358 - val_accuracy: 0.8025\n",
      "Epoch 10/10\n",
      "323432/323432 [==============================] - 236s 728us/step - loss: 0.3326 - accuracy: 0.8531 - val_loss: 0.4391 - val_accuracy: 0.8044\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(model_params)\n",
    "model.initialize()\n",
    "_ = model.fit(X_train_, y_train_, X_valid_, y_valid_)\n",
    "model.save('./siamese_network.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Way to Calculate Similarity\n",
    "\n",
    "We can also get the encoding output from LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network = model._model\n",
    "get_lstm_output = K.function([siamese_network.layers[0].input,\n",
    "                              siamese_network.layers[1].input],\n",
    "                             [siamese_network.layers[3].get_output_at(0),\n",
    "                              siamese_network.layers[3].get_output_at(1)])\n",
    "\n",
    "def calculate_lstm_encoding_similarity(idx):\n",
    "    sentence_1 = X_train_q1_[idx].reshape(-1, 1)\n",
    "    sentence_2 = X_train_q2_[idx].reshape(-1, 1)\n",
    "    lstm_output = get_lstm_output([sentence_1, sentence_2])\n",
    "    \n",
    "    sentence_1_encoding = lstm_output[0][:, -1]\n",
    "    sentence_2_encoding = lstm_output[1][:, -1]\n",
    "    return 1 - cosine(sentence_1_encoding, sentence_2_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 0 - LSTM Encoding Similarity = 0.8509\n",
      "True label: 0 - LSTM Encoding Similarity = 0.7504\n",
      "True label: 0 - LSTM Encoding Similarity = 0.4040\n",
      "True label: 0 - LSTM Encoding Similarity = 0.9986\n",
      "True label: 0 - LSTM Encoding Similarity = 0.6755\n",
      "True label: 1 - LSTM Encoding Similarity = 0.4051\n",
      "True label: 0 - LSTM Encoding Similarity = 0.9503\n",
      "True label: 1 - LSTM Encoding Similarity = 0.5221\n",
      "True label: 0 - LSTM Encoding Similarity = 0.3798\n",
      "True label: 0 - LSTM Encoding Similarity = 0.9722\n",
      "True label: 0 - LSTM Encoding Similarity = 0.5528\n",
      "True label: 1 - LSTM Encoding Similarity = 0.9219\n",
      "True label: 1 - LSTM Encoding Similarity = 0.9948\n",
      "True label: 1 - LSTM Encoding Similarity = 0.9901\n",
      "True label: 0 - LSTM Encoding Similarity = 0.1513\n",
      "True label: 1 - LSTM Encoding Similarity = 0.3399\n",
      "True label: 1 - LSTM Encoding Similarity = 0.8977\n",
      "True label: 0 - LSTM Encoding Similarity = 0.9150\n",
      "True label: 1 - LSTM Encoding Similarity = 0.9412\n",
      "True label: 0 - LSTM Encoding Similarity = 0.5148\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "for idx in range(20):\n",
    "    score = calculate_lstm_encoding_similarity(idx)\n",
    "    label = train['is_duplicate'].iloc[idx]\n",
    "    print(f'True label: {label} - LSTM Encoding Similarity = {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
