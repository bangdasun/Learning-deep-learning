{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "\n",
    "Attention mechanism was first published in [Bahdanau et al, Neural Machine Translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf). Where it is applied in a Seq2Seq model as a \"layer\".\n",
    "\n",
    "Seq2Seq models have **encoder** and **decoder** part. Attention layer adds extra info from the encoder contexts to decoder. Using Andrew Ng's notation, $(h_1, h_2, \\cdots, h_n)$ is the hidden state (output) from the encoder part (say a LSTM layer), $h_n$ is the final state of encoder part and initial state of the decoder part, for decoder part it is denoted as $s_0$, the $i$ th output (translate word) $s_i$ is $f(s_{i-1}, y_{i-1})$ where $f$ denote the feed forward steps. Now with attention mechanism, some context $c$ is extracted from the encoder part, then also as input for the decoder part: $s_i = f(s_{i-1}, y_{i-1}, c_i)$. $c_i$ is a weighted average of encoder output hidden states:\n",
    "\n",
    "$$\n",
    "c_i = \\sum^{n}_{j=1}\\alpha_{ij}h_j,\n",
    "$$\n",
    "\n",
    "here the weight $\\alpha_{ij}$ denote the **\"attention\"** that the $i$ th output should pay on $j$ th input. And \n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum^{n}_{k=1}\\exp(e_{ik})},\n",
    "$$\n",
    "\n",
    "where $e_{ij}$ is calculated from previous decoder hidden state $s_{i-1}$ and $j$ th encoder output hidden state $h_{j}$ use a simple layer (through one activation function).\n",
    "\n",
    "\n",
    "### Apply to Other Model Structures\n",
    "\n",
    "Idea of attention mechanism could also be applied to other structures. I used it several times in kaggle competitions. Therefore I just clean and summarize the code from kaggle kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\Bangda\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Implementation\n",
    "\n",
    "https://keras.io/layers/writing-your-own-keras-layers/\n",
    "\n",
    "- `build(input_shape)`: define weights, must set `self.built = True` at the end.\n",
    "- `call(x)`: define layer logic.\n",
    "- `compute_output_shape(input_shape)`: specify in case the layer modifies the shape of input.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Attention layer used in feed forward structure\n",
    "    \n",
    "    Refer from\n",
    "    author: @qqgeogor\n",
    "    kaggle profile: @https://www.kaggle.com/qqgeogor\n",
    "    kernel: https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "    \n",
    "    Originally the idea from:\n",
    "    https://arxiv.org/pdf/1512.08756.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, step_dim, bias=True,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        :param step_dim : int, number of timestamps to use. If it's after RNN layer it will be max_len.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.step_dim = step_dim\n",
    "        self.bias = bias\n",
    "        self.feature_dim = 0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \"\"\" define weights \"\"\"\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1], ),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.feature_dim = input_shape[-1]\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1], ),\n",
    "                                      initializer='zero',\n",
    "                                      name='{}_b'.format(self.name),\n",
    "                                      regularizer=self.b_regularizer,\n",
    "                                      constraint=self.b_constraint)\n",
    "        \n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        \"\"\" define structure \"\"\"\n",
    "        # e_ij = W * input + b\n",
    "        e_ij = K.reshape(K.dot(K.reshape(x, (-1, self.feature_dim)), \n",
    "                                         K.reshape(self.W, (self.feature_dim, 1))),\n",
    "                         (-1, self.step_dim))\n",
    "        if self.bias:\n",
    "            e_ij = e_ij + self.b\n",
    "        \n",
    "        e_ij = K.tanh(e_ij)\n",
    "        a = K.exp(e_ij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * K.cast(mask, K.floatx())\n",
    "        \n",
    "        # softmax normalization\n",
    "        a = a / K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        \n",
    "        # output context\n",
    "        weighted_input = x * a\n",
    "        c = K.sum(weighted_input, axis=1)\n",
    "        return c\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Implementation\n",
    "\n",
    "Very neat. Just need to define `__init__()` and `foward()` method like what needed for other neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.bias = bias\n",
    "        \n",
    "        W = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform(W)\n",
    "        self.W = nn.Parameter(W)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "            \n",
    "    def forward(self, x, mask=None):\n",
    "        e_ij = torch.mm(x.contiguous().view(-1, self.feature_dim), self.W).view(-1, self.step_dim)\n",
    "        if self.bias:\n",
    "            e_ij = e_ij + self.b\n",
    "        \n",
    "        e_ij = torch.tanh(e_ij)\n",
    "        a = torch.exp(e_ij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "        \n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsequeeze(a, -1)\n",
    "        c = torch.sum(weighted_input, 1)\n",
    "        return c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
